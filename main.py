from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import numpy as np
import joblib
import os
import gc
import tensorflow as tf 

# --------- ì„¤ì • ---------
MODEL_KEYS = ["hangul", "digit"]

app = FastAPI()

# --------- CORS ì„¤ì • ---------
origins = [
    "http://localhost:5173",
]
origin_regex = r"https://.*\.vercel\.app"

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_origin_regex=origin_regex,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# âœ… [ì¶”ê°€ë¨] ë£¨íŠ¸ ê²½ë¡œ ì ‘ì† ì‹œ 404 ë°©ì§€ìš© (Render í—¬ìŠ¤ ì²´í¬ í†µê³¼ìš©)
@app.get("/")
def home():
    return {"message": "Smart Sign Language Server (TFLite) is Running!"}

@app.head("/")
def keep_alive():
    return {"message": "I am alive"}

# --------- ëª¨ë¸ ë¡œë“œ (TFLite ë²„ì „) ---------
interpreters = {}
input_details = {}
output_details = {}
scalers = {}
encoders = {}

@app.on_event("startup")
def load_models():
    print("ğŸš€ [STARTUP] TFLite ëª¨ë¸ ë¡œë”© ì‹œì‘ (ì´ˆê²½ëŸ‰ ëª¨ë“œ)...")
    
    for key in MODEL_KEYS:
        try:
            path = f"model_{key}.tflite"
            
            if os.path.exists(path):
                interpreter = tf.lite.Interpreter(model_path=path)
                interpreter.allocate_tensors()
                
                interpreters[key] = interpreter
                input_details[key] = interpreter.get_input_details()
                output_details[key] = interpreter.get_output_details()
                
                scalers[key] = joblib.load(f"scaler_{key}.pkl")
                encoders[key] = joblib.load(f"label_encoder_{key}.pkl")
                
                print(f"   âœ… Loaded {key} (TFLite) successfully.")
                gc.collect()
            else:
                print(f"   âš ï¸ TFLite file not found: {path}")
        except Exception as e:
            print(f"   âŒ Failed to load {key}: {e}")

class PredictIn(BaseModel):
    model_key: str
    features: list[float]

@app.post("/predict")
def predict(inp: PredictIn):
    try:
        if inp.model_key not in interpreters:
            return {"label": "ì¤€ë¹„ì¤‘", "confidence": 0.0}

        interpreter = interpreters[inp.model_key]
        scaler = scalers[inp.model_key]
        encoder = encoders[inp.model_key]
        in_det = input_details[inp.model_key]
        out_det = output_details[inp.model_key]

        features_arr = np.array(inp.features, dtype=np.float32).reshape(1, -1)
        x = scaler.transform(features_arr)
        x = x.astype(np.float32)

        interpreter.set_tensor(in_det[0]['index'], x)
        interpreter.invoke()
        
        y = interpreter.get_tensor(out_det[0]['index'])[0]
        
        idx = int(np.argmax(y))
        label = encoder.inverse_transform([idx])[0]
        confidence = float(y[idx])

        return {"label": label, "confidence": confidence}

    except Exception as e:
        print(f"âŒ Error: {e}")
        return {"label": "Error", "confidence": 0.0, "detail": str(e)}