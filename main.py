from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import numpy as np
import joblib
import os
import tensorflow as tf
from typing import List, Union

# --------- ÏÑ§Ï†ï ---------
MODEL_KEYS = ["hangul", "digit", "word"] # 'word' Ìè¨Ìï®

app = FastAPI()

# --------- CORS ÏÑ§Ï†ï ---------
origins = ["*"] # Í∞úÎ∞ú Ìé∏ÏùòÎ•º ÏúÑÌï¥ Ï†ÑÏ≤¥ ÌóàÏö© (Î∞∞Ìè¨ Ïãú ÏàòÏ†ï Í∂åÏû•)

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
def home():
    return {"message": "Server is Running!"}

# --------- Î™®Îç∏ Î°úÎìú ---------
interpreters = {}
input_details = {}
output_details = {}
scalers = {}
encoders = {}

@app.on_event("startup")
def load_models():
    print("üöÄ Î™®Îç∏ Î°úÎî© ÏãúÏûë...")
    for key in MODEL_KEYS:
        try:
            path = f"model_{key}.tflite"
            if os.path.exists(path):
                interpreter = tf.lite.Interpreter(model_path=path)
                interpreter.allocate_tensors()
                interpreters[key] = interpreter
                input_details[key] = interpreter.get_input_details()
                output_details[key] = interpreter.get_output_details()
                scalers[key] = joblib.load(f"scaler_{key}.pkl")
                encoders[key] = joblib.load(f"label_encoder_{key}.pkl")
                print(f"   ‚úÖ {key} Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µ")
            else:
                print(f"   ‚ö†Ô∏è ÌååÏùº ÏóÜÏùå: {path}")
        except Exception as e:
            print(f"   ‚ùå {key} Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}")

# ÏöîÏ≤≠ Îç∞Ïù¥ÌÑ∞ ÌòïÏãù Ï†ïÏùò
# featuresÎäî 1Ï∞®Ïõê Î¶¨Ïä§Ìä∏(Í∏∞Ï°¥)Ïùº ÏàòÎèÑ ÏûàÍ≥†, 2Ï∞®Ïõê Î¶¨Ïä§Ìä∏(ÏÉà Î™®Îç∏, 90x258)Ïùº ÏàòÎèÑ ÏûàÏùå
class PredictIn(BaseModel):
    model_key: str
    features: Union[List[float], List[List[float]]] 

@app.post("/predict")
def predict(inp: PredictIn):
    try:
        if inp.model_key not in interpreters:
            return {"label": "Error", "detail": "Î™®Îç∏ ÏóÜÏùå"}

        interpreter = interpreters[inp.model_key]
        # scaler = scalers[inp.model_key] # ÏÉà Î™®Îç∏ÏùÄ Ïä§ÏºÄÏùºÎü¨ Ìå®Ïä§ (ÌïÑÏöîÏãú ÌôúÏÑ±Ìôî)
        encoder = encoders[inp.model_key]
        in_det = input_details[inp.model_key]
        out_det = output_details[inp.model_key]

        # Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò Î°úÏßÅ
        if inp.model_key == "word":
            # (90, 258) -> (1, 90, 258) ÌòïÌÉúÎ°ú Î≥ÄÌôò
            data = np.array(inp.features, dtype=np.float32)
            data = np.expand_dims(data, axis=0) 
        else:
            # Í∏∞Ï°¥ Î™®Îç∏: (22,) -> (1, 22)
            scaler = scalers[inp.model_key]
            data = np.array(inp.features, dtype=np.float32).reshape(1, -1)
            data = scaler.transform(data).astype(np.float32)

        # Ï∂îÎ°† Ïã§Ìñâ
        interpreter.set_tensor(in_det[0]['index'], data)
        interpreter.invoke()
        
        # Í≤∞Í≥º Ìï¥ÏÑù
        y = interpreter.get_tensor(out_det[0]['index'])[0]
        idx = int(np.argmax(y))
        label = encoder.inverse_transform([idx])[0]
        confidence = float(y[idx])

        return {"label": label, "confidence": confidence}

    except Exception as e:
        print(f"‚ùå Error: {e}")
        return {"label": "Error", "detail": str(e)}