# main.py
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import numpy as np
import joblib
import os
import gc
import tensorflow as tf # TFLite Interpreter ì‚¬ìš©

# --------- ì„¤ì • ---------
MODEL_KEYS = ["hangul", "digit"]

app = FastAPI()

# --------- CORS ì„¤ì • ---------
origins = [
    "http://localhost:5173",
]
origin_regex = r"https://.*\.vercel\.app"

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_origin_regex=origin_regex,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --------- ëª¨ë¸ ë¡œë“œ (TFLite ë²„ì „) ---------
interpreters = {} # ëª¨ë¸ ëŒ€ì‹  ì¸í„°í”„ë¦¬í„° ì €ì¥
input_details = {}
output_details = {}
scalers = {}
encoders = {}

@app.on_event("startup")
def load_models():
    print("ğŸš€ [STARTUP] TFLite ëª¨ë¸ ë¡œë”© ì‹œì‘ (ì´ˆê²½ëŸ‰ ëª¨ë“œ)...")
    
    for key in MODEL_KEYS:
        try:
            # .tflite íŒŒì¼ ê²½ë¡œ
            path = f"model_{key}.tflite"
            
            if os.path.exists(path):
                # 1. ì¸í„°í”„ë¦¬í„° ë¡œë“œ (Keras model.load_modelë³´ë‹¤ í›¨ì”¬ ê°€ë²¼ì›€)
                interpreter = tf.lite.Interpreter(model_path=path)
                interpreter.allocate_tensors()
                
                interpreters[key] = interpreter
                
                # ì…ì¶œë ¥ ì •ë³´ ì €ì¥ (ë‚˜ì¤‘ì— predictí•  ë•Œ í•„ìš”)
                input_details[key] = interpreter.get_input_details()
                output_details[key] = interpreter.get_output_details()
                
                # ìŠ¤ì¼€ì¼ëŸ¬/ì¸ì½”ë” ë¡œë“œ
                scalers[key] = joblib.load(f"scaler_{key}.pkl")
                encoders[key] = joblib.load(f"label_encoder_{key}.pkl")
                
                print(f"   âœ… Loaded {key} (TFLite) successfully.")
                gc.collect()
            else:
                print(f"   âš ï¸ TFLite file not found: {path} (ë³€í™˜í–ˆë‚˜ìš”?)")
        except Exception as e:
            print(f"   âŒ Failed to load {key}: {e}")

class PredictIn(BaseModel):
    model_key: str
    features: list[float]

@app.post("/predict")
def predict(inp: PredictIn):
    try:
        if inp.model_key not in interpreters:
            return {"label": "ì¤€ë¹„ì¤‘", "confidence": 0.0}

        interpreter = interpreters[inp.model_key]
        scaler = scalers[inp.model_key]
        encoder = encoders[inp.model_key]
        in_det = input_details[inp.model_key]
        out_det = output_details[inp.model_key]

        # 1. ë°ì´í„° ì „ì²˜ë¦¬
        features_arr = np.array(inp.features, dtype=np.float32).reshape(1, -1)
        x = scaler.transform(features_arr)
        x = x.astype(np.float32) # TFLiteëŠ” íƒ€ì…ì— ë¯¼ê°í•¨

        # 2. ì¶”ë¡  ì‹¤í–‰ (Invoke)
        interpreter.set_tensor(in_det[0]['index'], x)
        interpreter.invoke() # ì‹¤í–‰!
        
        # 3. ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        y = interpreter.get_tensor(out_det[0]['index'])[0]
        
        idx = int(np.argmax(y))
        label = encoder.inverse_transform([idx])[0]
        confidence = float(y[idx])

        return {"label": label, "confidence": confidence}

    except Exception as e:
        print(f"âŒ Error: {e}")
        return {"label": "Error", "confidence": 0.0, "detail": str(e)}